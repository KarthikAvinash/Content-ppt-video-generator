{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pptxtopdf PyMuPDF Pillow moviepy python-dotenv\n",
    "# %pip install -q -U google-generativeai\n",
    "# %pip install numpy PyMuPDF moviepy pptxtopdf mediapipe opencv-python azure-cognitiveservices-speech\n",
    "# %pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IISc-Internship\\Week11\\info_to_pptx\\kar\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "from pptxtopdf import convert\n",
    "import google.generativeai as genai\n",
    "from moviepy.editor import *\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from moviepy.editor import concatenate_audioclips, AudioFileClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the env variables consisting of API Keys and area\n",
    "* make a .env file in the current working directory\n",
    "* add following variables:\n",
    "    -   SPEECH_SERVICEE_REGION\n",
    "    -   SPEECH_SERVICE_API_KEY\n",
    "    -   GOOGLE_GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_dotenv('./.env'): raise Exception(\"env file not found\")\n",
    "\n",
    "SERVICE_REGION = os.getenv(\"SPEECH_SERVICEE_REGION\")\n",
    "SUBSCRIPTION_KEY = os.getenv(\"SPEECH_SERVICE_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_GEMINI_API_KEY\")\n",
    "\n",
    "url_base = f\"https://{SERVICE_REGION}.customvoice.api.speech.microsoft.com/api\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "MODEL = \"gemini-1.5-flash-latest\" # CAN ALSO USE: \"gemini-1.5-pro-latest\" - BUT WILL BE A BIT SLOW.\n",
    "model = genai.GenerativeModel(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, output_file):\n",
    "    speech_key = SUBSCRIPTION_KEY # Replace with your Speech service key\n",
    "    service_region = SERVICE_REGION   # Replace with your Speech service region\n",
    "    \n",
    "    # Create a speech configuration object\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_config.speech_synthesis_voice_name = 'en-US-AndrewNeural'\n",
    "    \n",
    "    # Create a speech synthesizer object with a file output\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(filename=output_file)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Synthesize the text and save it to the file\n",
    "    result = synthesizer.speak_text_async(text).get()\n",
    "    \n",
    "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(f'Text-to-speech conversion successful. Audio saved to {output_file}')\n",
    "    else:\n",
    "        print(f'Text-to-speech conversion failed: {result.reason}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File and Directories names\n",
    "* can change to desired names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ppt = \"output.pptx\" # Make sure it in in the current working directory (Example: file_name.pptx)\n",
    "# holds content\n",
    "image_names = []\n",
    "slides_content = []\n",
    "# directories\n",
    "images_dir = \"images\"\n",
    "avtr_video_dir = \"avt_videos\"\n",
    "# output\n",
    "frames_video_file = \"main.mp4\"\n",
    "avtr_video_file = \"avtr_vid.webm\"\n",
    "final_video_file = \"final.webm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder_and_create(path):\n",
    "    if os.path.exists(path): shutil.rmtree(path)\n",
    "    os.makedirs(path)\n",
    "\n",
    "def convert_to_images(input_file_path):\n",
    "    file_name = input_file_path.split(\".\")[0]\n",
    "    convert(input_file_path, \"\")\n",
    "    pdf_path = file_name + \".pdf\"\n",
    "    if os.path.exists(images_dir): shutil.rmtree(images_dir)\n",
    "    os.makedirs(images_dir)\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    num_pages = len(pdf_document)\n",
    "    num_digits = len(str(num_pages))\n",
    "    for page_num in range(num_pages):\n",
    "        page_number_str = str(page_num + 1).zfill(num_digits)\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        img_path = f\"{images_dir}/page_{page_number_str}.png\"\n",
    "        image_names.append(f\"page_{page_number_str}\")\n",
    "        img.save(img_path)\n",
    "        print(f\"Saved {img_path}\")\n",
    "\n",
    "def download_file(url, local_path, index):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        # filename = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "        filename = image_names[index]+\".webm\"\n",
    "        local_filename = os.path.join(local_path, filename)\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                if chunk: f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def preprocess_response(text):\n",
    "    text = text.replace('*', '')\n",
    "    return text\n",
    "\n",
    "def combine_webm_videos(output_file):\n",
    "    clips = []\n",
    "    for file in os.listdir(avtr_video_dir):\n",
    "        path = os.path.join(avtr_video_dir,file)\n",
    "        clips.append(VideoFileClip(path))\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_clip.write_videofile(output_file, codec=\"libvpx\", audio_codec=\"libvorbis\")\n",
    "    final_clip.close()\n",
    "    for clip in clips: clip.close()\n",
    "\n",
    "def get_video_duration(file_path):\n",
    "    video = VideoFileClip(file_path)\n",
    "    duration = video.duration\n",
    "    video.close()\n",
    "    return duration\n",
    "\n",
    "def custom_resize(clip, newsize):\n",
    "    def resize_frame(frame):\n",
    "        img = Image.fromarray(frame)\n",
    "        return np.array(img.resize(newsize, Image.LANCZOS))\n",
    "    return clip.fl_image(resize_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: It deletes and creates all previous generated intermediate folders and files!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_and_create(images_dir)\n",
    "delete_folder_and_create(avtr_video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the .pptx into images in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Output file 'd:\\IISc-Internship\\Week11\\info_to_pptx\\output.pdf' already exists.\n",
      "Conversion completed: 0 files converted successfully, 1 files failed.\n",
      "Saved images/page_1.png\n",
      "Saved images/page_2.png\n",
      "Saved images/page_3.png\n"
     ]
    }
   ],
   "source": [
    "convert_to_images(input_ppt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate content for each slide based on the content of previous slides\n",
    "* Previous slide's generated content is also sent for each of the slide so that the context is preserved.\n",
    "* prompt could be changed, currently using a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide: 1\n",
      "This presentation will serve as a comprehensive guide to Large Language Model (LLM) agents, explaining their capabilities in solving intricate problems. We will explore their components, practical use cases, and the challenges that come with their implementation.  Think of LLM agents as sophisticated tools equipped with reasoning, data analysis, and planning capabilities to solve complex problems in areas like language processing and natural language understanding. \n",
      "\n",
      "Slide: 2\n",
      "This slide highlights the key distinction between traditional Large Language Models (LLMs) and LLM agents. While basic LLMs with Retrieval Augmented Generation (RAG) systems primarily retrieve information, LLM agents go beyond simple retrieval. They employ sequential reasoning, planning, and memory to analyze data, plan tasks, and connect information to solve complex problems, as illustrated by the flowchart depicting the components of an LLM agent.  For example, consider a medical diagnosis scenario. While a basic LLM might retrieve symptoms and related medical conditions, an LLM agent could analyze patient history, medical records, and current symptoms to arrive at a more accurate diagnosis, potentially even suggesting a treatment plan. \n",
      "\n",
      "Slide: 3\n",
      "This slide breaks down the building blocks of LLM agents, highlighting three key components: the Agent/Brain, Memory, and Planning. The Agent/Brain is the core language model, while Memory enables short-term and long-term retention, allowing for tailored responses and personalized interactions. Planning, the final component, focuses on formulating and refining plans, breaking down complex tasks into smaller steps. This allows LLM agents to adapt to real-world situations and utilize reasoning methods like Chain of Thought (CoT) and Tree of Thought (ToT). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# You are a professional presenter who has created this slides and is explaining to audience. You need to look into the current slide of a PowerPoint presentation given to you as an image and explain it to the audience. Ensure the explanation is clear, detailed, and relevant to the content of the slide. Do not use any greetings like good morning, evening, or night. Do not use astricks in the response. The output should be plain text without any markdown notations. Explain the slides slide by slide. If previous slides have been explained, continue seamlessly from the previous slide's information.\n",
    "# \"\"\"\n",
    "\n",
    "# for idx,slide in enumerate(os.listdir(images_dir)):\n",
    "#     print(\"Slide: \", str(idx+1))\n",
    "#     img = PIL.Image.open(os.path.join(\"images\", slide))\n",
    "#     prompt = prompt_template\n",
    "#     if len(slides_content) > 0:\n",
    "#         prompt += \" Continue from the previous slide's information given here: \"\n",
    "#         for j, text in enumerate(slides_content):\n",
    "#             prompt += f\"Slide-{j+1} {text} \"\n",
    "#     prompt += f\"Explain the current slide (Slide-{len(slides_content)+1}) in detail:\"\n",
    "#     response = model.generate_content([prompt, img], stream=True)\n",
    "#     response.resolve()\n",
    "#     description = preprocess_response(response.text)\n",
    "#     slides_content.append(description)\n",
    "#     print(description)\n",
    "\n",
    "slides_content = []\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a professional presenter explaining a PowerPoint presentation to an audience. Analyze the current slide image and provide a concise, insightful explanation. Focus on the key points and main ideas rather than reading any text verbatim from the slide. Your explanation should be brief but informative, suitable for a quick presentation.\n",
    "\n",
    "Guidelines:\n",
    "1. Do not use greetings or introductions.\n",
    "2. Avoid reading sentences directly from the slide.\n",
    "3. Provide context and insights beyond what's visibly written.\n",
    "4. Keep the explanation concise, aiming for about 2-3 sentences per slide.\n",
    "5. Use plain text without any special formatting or markdown.\n",
    "6. If this isn't the first slide, ensure your explanation flows naturally from the previous content.\n",
    "7. Use examples other than the one given in the ppt.\n",
    "8. Provide additional details when and where required.\n",
    "9. Connect the contents from previous slides also.\n",
    "10. Be as concise as possible.\n",
    "11. Focus more on explaining images in each slide which may be flowcharts or images provided that they are technical images with information related to the topic.\n",
    "\n",
    "Current slide number: {slide_number}\n",
    "\n",
    "Previous slides' content (if any):\n",
    "{previous_slides_content}\n",
    "\n",
    "Explain this slide succinctly:\n",
    "\"\"\"\n",
    "\n",
    "for idx, slide in enumerate(os.listdir(images_dir)):\n",
    "    print(f\"Slide: {idx+1}\")\n",
    "    img = PIL.Image.open(os.path.join(\"images\", slide))\n",
    "    \n",
    "    previous_slides_content = \"\"\n",
    "    if len(slides_content) > 0:\n",
    "        previous_slides_content = \" \".join([f\"Slide-{j+1}: {text}\" for j, text in enumerate(slides_content)])\n",
    "    \n",
    "    prompt = prompt_template.format(\n",
    "        slide_number=idx+1,\n",
    "        previous_slides_content=previous_slides_content\n",
    "    )\n",
    "    # print(prompt)\n",
    "    # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n\\n\")\n",
    "    \n",
    "    response = model.generate_content([prompt, img], stream=True)\n",
    "    response.resolve()\n",
    "    description = preprocess_response(response.text)\n",
    "    slides_content.append(description)\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This presentation will serve as a comprehensive guide to Large Language Model (LLM) agents, explaining their capabilities in solving intricate problems. We will explore their components, practical use cases, and the challenges that come with their implementation.  Think of LLM agents as sophisticated tools equipped with reasoning, data analysis, and planning capabilities to solve complex problems in areas like language processing and natural language understanding. \\n',\n",
       " 'This slide highlights the key distinction between traditional Large Language Models (LLMs) and LLM agents. While basic LLMs with Retrieval Augmented Generation (RAG) systems primarily retrieve information, LLM agents go beyond simple retrieval. They employ sequential reasoning, planning, and memory to analyze data, plan tasks, and connect information to solve complex problems, as illustrated by the flowchart depicting the components of an LLM agent.  For example, consider a medical diagnosis scenario. While a basic LLM might retrieve symptoms and related medical conditions, an LLM agent could analyze patient history, medical records, and current symptoms to arrive at a more accurate diagnosis, potentially even suggesting a treatment plan. \\n',\n",
       " 'This slide breaks down the building blocks of LLM agents, highlighting three key components: the Agent/Brain, Memory, and Planning. The Agent/Brain is the core language model, while Memory enables short-term and long-term retention, allowing for tailored responses and personalized interactions. Planning, the final component, focuses on formulating and refining plans, breaking down complex tasks into smaller steps. This allows LLM agents to adapt to real-world situations and utilize reasoning methods like Chain of Thought (CoT) and Tree of Thought (ToT). \\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slides_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_content=['This presentation will serve as a comprehensive guide to Large Language Model (LLM) agents, explaining their capabilities in solving intricate problems. We will explore their components, practical use cases, and the challenges that come with their implementation.  Think of LLM agents as sophisticated tools equipped with reasoning, data analysis, and planning capabilities to solve complex problems in areas like language processing and natural language understanding. \\n',\n",
    " 'This slide highlights the key distinction between traditional Large Language Models (LLMs) and LLM agents. While basic LLMs with Retrieval Augmented Generation (RAG) systems primarily retrieve information, LLM agents go beyond simple retrieval. They employ sequential reasoning, planning, and memory to analyze data, plan tasks, and connect information to solve complex problems, as illustrated by the flowchart depicting the components of an LLM agent.  For example, consider a medical diagnosis scenario. While a basic LLM might retrieve symptoms and related medical conditions, an LLM agent could analyze patient history, medical records, and current symptoms to arrive at a more accurate diagnosis, potentially even suggesting a treatment plan. \\n',\n",
    " 'This slide breaks down the building blocks of LLM agents, highlighting three key components: the Agent/Brain, Memory, and Planning. The Agent/Brain is the core language model, while Memory enables short-term and long-term retention, allowing for tailored responses and personalized interactions. Planning, the final component, focuses on formulating and refining plans, breaking down complex tasks into smaller steps. This allows LLM agents to adapt to real-world situations and utilize reasoning methods like Chain of Thought (CoT) and Tree of Thought (ToT). \\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slides:  [{'slide_number': 1, 'sentences': ['This presentation will serve as a comprehensive guide to Large Language Model (LLM) agents, explaining their capabilities in solving intricate problems.', 'We will explore their components, practical use cases, and the challenges that come with their implementation.', 'Think of LLM agents as sophisticated tools equipped with reasoning, data analysis, and planning capabilities to solve complex problems in areas like language processing and natural language understanding.']}, {'slide_number': 2, 'sentences': ['This slide highlights the key distinction between traditional Large Language Models (LLMs) and LLM agents.', 'While basic LLMs with Retrieval Augmented Generation (RAG) systems primarily retrieve information, LLM agents go beyond simple retrieval.', 'They employ sequential reasoning, planning, and memory to analyze data, plan tasks, and connect information to solve complex problems, as illustrated by the flowchart depicting the components of an LLM agent.', 'For example, consider a medical diagnosis scenario.', 'While a basic LLM might retrieve symptoms and related medical conditions, an LLM agent could analyze patient history, medical records, and current symptoms to arrive at a more accurate diagnosis, potentially even suggesting a treatment plan.']}, {'slide_number': 3, 'sentences': ['This slide breaks down the building blocks of LLM agents, highlighting three key components: the Agent/Brain, Memory, and Planning.', 'The Agent/Brain is the core language model, while Memory enables short-term and long-term retention, allowing for tailored responses and personalized interactions.', 'Planning, the final component, focuses on formulating and refining plans, breaking down complex tasks into smaller steps.', 'This allows LLM agents to adapt to real-world situations and utilize reasoning methods like Chain of Thought (CoT) and Tree of Thought (ToT).']}]\n",
      "Slide 1:\n",
      "  - This presentation will serve as a comprehensive guide to Large Language Model (LLM) agents, explaining their capabilities in solving intricate problems.\n",
      "  - We will explore their components, practical use cases, and the challenges that come with their implementation.\n",
      "  - Think of LLM agents as sophisticated tools equipped with reasoning, data analysis, and planning capabilities to solve complex problems in areas like language processing and natural language understanding.\n",
      "Slide 2:\n",
      "  - This slide highlights the key distinction between traditional Large Language Models (LLMs) and LLM agents.\n",
      "  - While basic LLMs with Retrieval Augmented Generation (RAG) systems primarily retrieve information, LLM agents go beyond simple retrieval.\n",
      "  - They employ sequential reasoning, planning, and memory to analyze data, plan tasks, and connect information to solve complex problems, as illustrated by the flowchart depicting the components of an LLM agent.\n",
      "  - For example, consider a medical diagnosis scenario.\n",
      "  - While a basic LLM might retrieve symptoms and related medical conditions, an LLM agent could analyze patient history, medical records, and current symptoms to arrive at a more accurate diagnosis, potentially even suggesting a treatment plan.\n",
      "Slide 3:\n",
      "  - This slide breaks down the building blocks of LLM agents, highlighting three key components: the Agent/Brain, Memory, and Planning.\n",
      "  - The Agent/Brain is the core language model, while Memory enables short-term and long-term retention, allowing for tailored responses and personalized interactions.\n",
      "  - Planning, the final component, focuses on formulating and refining plans, breaking down complex tasks into smaller steps.\n",
      "  - This allows LLM agents to adapt to real-world situations and utilize reasoning methods like Chain of Thought (CoT) and Tree of Thought (ToT).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Initialize the list that will hold the structured data\n",
    "sent_slide_content = []\n",
    "\n",
    "# Iterate over each slide's text\n",
    "for i, slide_text in enumerate(slides_content):\n",
    "    # Split the slide text into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', slide_text.strip())\n",
    "    # print(\"sentences: \", sentences)\n",
    "    trimmed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        trimmed_sentences.append(\" \".join(sentence.split()))\n",
    "    # Create a dictionary for the current slide\n",
    "    slide_dict = {\n",
    "        \"slide_number\": i + 1,\n",
    "        \"sentences\": trimmed_sentences\n",
    "    }\n",
    "\n",
    "    # Add the dictionary to the slides list\n",
    "    sent_slide_content.append(slide_dict)\n",
    "\n",
    "print(\"slides: \",sent_slide_content)\n",
    "# Output the structured data\n",
    "for slide in sent_slide_content:\n",
    "    print(f\"Slide {slide['slide_number']}:\")\n",
    "    for sentence in slide['sentences']:\n",
    "        print(f\"  - {sentence}\")\n",
    "\n",
    "# Now you have the structured slides list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILES = \"audios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(AUDIO_FILES):\n",
    "    shutil.rmtree(AUDIO_FILES)\n",
    "    os.mkdir(AUDIO_FILES)\n",
    "else: os.mkdir(AUDIO_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech conversion successful. Audio saved to audios\\slide001_sentence001.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide001_sentence002.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide001_sentence003.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide002_sentence001.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide002_sentence002.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide002_sentence003.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide002_sentence004.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide002_sentence005.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide003_sentence001.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide003_sentence002.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide003_sentence003.wav\n",
      "Text-to-speech conversion successful. Audio saved to audios\\slide003_sentence004.wav\n"
     ]
    }
   ],
   "source": [
    "for slide in sent_slide_content:\n",
    "    slide_number = slide['slide_number']\n",
    "    for sentence_number, sentence in enumerate(slide['sentences'], start=1):\n",
    "        # Generate the file name in the format: \"slideXXX_sentenceYYY.wav\"\n",
    "        file_name = f\"slide{slide_number:03}_sentence{sentence_number:03}.wav\"\n",
    "        output_file_path = os.path.join(AUDIO_FILES, file_name)\n",
    "        \n",
    "        # Assuming text_to_speech is your function that generates the audio.\n",
    "        text_to_speech(sentence, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Here to generate the final video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:  ['slide001_sentence001.wav', 'slide001_sentence002.wav', 'slide001_sentence003.wav', 'slide002_sentence001.wav', 'slide002_sentence002.wav', 'slide002_sentence003.wav', 'slide003_sentence001.wav', 'slide003_sentence002.wav', 'slide003_sentence003.wav', 'slide003_sentence004.wav']\n",
      "Durations of each audio file: [6.64, 9.52, 8.04, 7.19, 8.42, 11.15, 6.27, 8.4, 12.32, 7.49]\n",
      "Combined durations for each slide: [24.2, 26.759999999999998, 34.480000000000004]\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from moviepy.editor import concatenate_audioclips, AudioFileClip\n",
    "import os\n",
    "\n",
    "def get_audio_durations(directory):\n",
    "    # List all MP3 or WAV files in the directory\n",
    "    files = sorted([f for f in os.listdir(directory) if f.endswith(('.mp3','.wav'))])\n",
    "    print(\"Files: \", files)\n",
    "    \n",
    "    # Initialize lists for clips, durations, and slide durations\n",
    "    clips = []\n",
    "    durations = []\n",
    "    slide_durations = []\n",
    "\n",
    "    current_slide = None\n",
    "    current_slide_duration = 0\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        # Load the audio file using pydub\n",
    "        audio_clip = AudioFileClip(file_path)\n",
    "        clips.append(audio_clip)\n",
    "        \n",
    "        # Calculate duration and add to the list\n",
    "        duration = audio_clip.duration\n",
    "        durations.append(duration)\n",
    "        \n",
    "        # Extract slide number from the file name\n",
    "        slide_number = int(file.split('_')[0][5:])\n",
    "\n",
    "        if current_slide is None:\n",
    "            current_slide = slide_number\n",
    "\n",
    "        if slide_number == current_slide:\n",
    "            current_slide_duration += duration\n",
    "        else:\n",
    "            slide_durations.append(current_slide_duration)\n",
    "            current_slide_duration = duration\n",
    "            current_slide = slide_number\n",
    "    \n",
    "    # Append the duration of the last slide\n",
    "    slide_durations.append(current_slide_duration)\n",
    "    \n",
    "    return durations, slide_durations\n",
    "\n",
    "# Usage example\n",
    "directory = \"processed_files/audios\"  # Replace with your directory path\n",
    "output_file = 'combined_audio.wav'  # Replace with your desired output file name\n",
    "durations, slide_durations = get_audio_durations(directory)\n",
    "\n",
    "print(\"Durations of each audio file:\", durations)\n",
    "print(\"Combined durations for each slide:\", slide_durations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageClip\n",
    "\n",
    "def create_video_from_image(image_path, output_video_path, duration, fps=30):\n",
    "    \"\"\"\n",
    "    Creates a video using a single image for a specific duration.\n",
    "\n",
    "    :param image_path: Path to the input image file.\n",
    "    :param output_video_path: Path where the output video will be saved.\n",
    "    :param duration: Duration of the video in seconds.\n",
    "    :param fps: Frames per second for the video. Default is 30.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image_clip = ImageClip(image_path)\n",
    "\n",
    "    # Set the duration of the video\n",
    "    video_clip = image_clip.set_duration(duration)\n",
    "\n",
    "    # Set the FPS (frames per second) of the video\n",
    "    video_clip = video_clip.set_fps(fps)\n",
    "\n",
    "    # Write the result to a video file\n",
    "    video_clip.write_videofile(output_video_path, codec=\"libx264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.2, 26.759999999999998, 34.480000000000004]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:33<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img source:  slide001.png\n",
      "slide number:  1\n",
      "slide_duration:  6.64\n",
      "Moviepy - Building video output_videos\\slide001_sentence001.mp4.\n",
      "Moviepy - Writing video output_videos\\slide001_sentence001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:34<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide001_sentence001.mp4\n",
      "img source:  slide001.png\n",
      "slide number:  1\n",
      "slide_duration:  9.52\n",
      "Moviepy - Building video output_videos\\slide001_sentence002.mp4.\n",
      "Moviepy - Writing video output_videos\\slide001_sentence002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:36<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide001_sentence002.mp4\n",
      "img source:  slide001.png\n",
      "slide number:  1\n",
      "slide_duration:  8.04\n",
      "Moviepy - Building video output_videos\\slide001_sentence003.mp4.\n",
      "Moviepy - Writing video output_videos\\slide001_sentence003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:38<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide001_sentence003.mp4\n",
      "img source:  slide002.png\n",
      "slide number:  2\n",
      "slide_duration:  7.19\n",
      "Moviepy - Building video output_videos\\slide002_sentence001.mp4.\n",
      "Moviepy - Writing video output_videos\\slide002_sentence001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:40<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide002_sentence001.mp4\n",
      "img source:  slide002.png\n",
      "slide number:  2\n",
      "slide_duration:  8.42\n",
      "Moviepy - Building video output_videos\\slide002_sentence002.mp4.\n",
      "Moviepy - Writing video output_videos\\slide002_sentence002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:42<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide002_sentence002.mp4\n",
      "img source:  slide002.png\n",
      "slide number:  2\n",
      "slide_duration:  11.15\n",
      "Moviepy - Building video output_videos\\slide002_sentence003.mp4.\n",
      "Moviepy - Writing video output_videos\\slide002_sentence003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:44<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide002_sentence003.mp4\n",
      "img source:  slide003.png\n",
      "slide number:  3\n",
      "slide_duration:  6.27\n",
      "Moviepy - Building video output_videos\\slide003_sentence001.mp4.\n",
      "Moviepy - Writing video output_videos\\slide003_sentence001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:46<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide003_sentence001.mp4\n",
      "img source:  slide003.png\n",
      "slide number:  3\n",
      "slide_duration:  8.4\n",
      "Moviepy - Building video output_videos\\slide003_sentence002.mp4.\n",
      "Moviepy - Writing video output_videos\\slide003_sentence002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:47<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide003_sentence002.mp4\n",
      "img source:  slide003.png\n",
      "slide number:  3\n",
      "slide_duration:  12.32\n",
      "Moviepy - Building video output_videos\\slide003_sentence003.mp4.\n",
      "Moviepy - Writing video output_videos\\slide003_sentence003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:50<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide003_sentence003.mp4\n",
      "img source:  slide003.png\n",
      "slide number:  3\n",
      "slide_duration:  7.49\n",
      "Moviepy - Building video output_videos\\slide003_sentence004.mp4.\n",
      "Moviepy - Writing video output_videos\\slide003_sentence004.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 2/200 [01:52<00:12, 15.62it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos\\slide003_sentence004.mp4\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"output_videos\")\n",
    "for idx, vid in enumerate(os.listdir(\"avt_videos\")):\n",
    "    img_source = vid[:8]+\".png\"\n",
    "    print(\"img source: \", img_source)\n",
    "    slide_number = int(img_source[5:8]) \n",
    "    print(\"slide number: \",slide_number)\n",
    "    slide_duration = durations[idx]\n",
    "    print(\"slide_duration: \",slide_duration)\n",
    "    image_path = os.path.join(\"images\",img_source)\n",
    "    output_video_path = os.path.join(\"output_videos\",vid)\n",
    "    create_video_from_image(image_path=image_path,output_video_path=output_video_path,duration=slide_duration, fps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import moviepy.editor as mpy\n",
    "from PIL import Image, ImageSequence\n",
    "import mediapipe as mp\n",
    "from moviepy.editor import ImageSequenceClip, VideoFileClip\n",
    "\n",
    "# Function to remove pixels where green value is greater than both red and blue values by a specified threshold\n",
    "def remove_pixels_based_on_green(image, threshold=5):\n",
    "    # Convert image to RGBA\n",
    "    image = image.convert(\"RGBA\")\n",
    "    data = image.getdata()\n",
    "\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        r, g, b, a = item\n",
    "        if (g - r >= threshold) and (g - b >= threshold):\n",
    "            new_data.append((0, 0, 0, 0))  # Make pixel transparent\n",
    "        else:\n",
    "            new_data.append(item)\n",
    "\n",
    "    image.putdata(new_data)\n",
    "    return image\n",
    "\n",
    "# Initialize MediaPipe Selfie Segmentation\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "def save_frames(input_video_path, threshold = 5):\n",
    "    if os.path.exists(\"frames\"):\n",
    "        shutil.rmtree(\"frames\")\n",
    "        os.mkdir(\"frames\")\n",
    "    else: os.mkdir(\"frames\")\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = 'frames'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    else:\n",
    "        shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Read the first frame to get dimensions\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read video\")\n",
    "        return\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    background = np.zeros((height, width, 3), dtype=np.uint8)  # Black background\n",
    "\n",
    "    # Get total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    for _ in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe\n",
    "        results = segmentation.process(RGB)\n",
    "        mask = results.segmentation_mask\n",
    "\n",
    "        # Create a binary mask for the foreground (foreground is where mask > 0.6)\n",
    "        mask = np.expand_dims(mask > 0.6, axis=-1)\n",
    "        mask = np.repeat(mask, 3, axis=-1)  # Convert to 3-channel\n",
    "\n",
    "        # Create the output frame with an alpha channel\n",
    "        output = np.zeros((height, width, 4), dtype=np.uint8)\n",
    "        output[:, :, :3] = np.where(mask, frame, background)  # Apply mask to frame\n",
    "        output[:, :, 3] = np.where(mask[:, :, 0], 255, 0)    # Set alpha channel based on mask\n",
    "\n",
    "        # Convert to PIL Image to process transparency and apply blur\n",
    "        pil_image = Image.fromarray(output, 'RGBA')\n",
    "        # pil_image = apply_gaussian_blur(pil_image)  # Apply Gaussian blur\n",
    "        pil_image = remove_pixels_based_on_green(pil_image, threshold)  # Remove pixels based on green value\n",
    "        output = np.array(pil_image)\n",
    "\n",
    "        # Save each frame as a PNG file\n",
    "        filename = os.path.join(output_dir, f\"frame_{frame_count:04d}.png\")\n",
    "        cv2.imwrite(filename, output)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def overlay_images_on_video(video_path, frames_dir, output_video_path, audio_path, overlay_height):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # List all image files in the directory and sort them\n",
    "    image_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.png')])\n",
    "    num_images = len(image_files)\n",
    "    \n",
    "    if num_images == 0:\n",
    "        print(\"Error: No images found in the directory.\")\n",
    "        return\n",
    "    \n",
    "    # Create a list to store output frames\n",
    "    output_frames = []\n",
    "\n",
    "    # Calculate the width of the overlay image maintaining its aspect ratio\n",
    "    overlay_width = int((overlay_height / height) * width)\n",
    "\n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to the start\n",
    "\n",
    "    for _ in tqdm(range(total_frames), desc=\"Processing Frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Determine the image to overlay\n",
    "        overlay_img_path = os.path.join(frames_dir, image_files[frame_count % num_images])\n",
    "        overlay_img = cv2.imread(overlay_img_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # Resize the overlay image to the desired dimensions while maintaining aspect ratio\n",
    "        if overlay_img.shape[0] != overlay_height:\n",
    "            aspect_ratio = overlay_img.shape[1] / overlay_img.shape[0]\n",
    "            new_width = int(overlay_height * aspect_ratio)\n",
    "            overlay_img = cv2.resize(overlay_img, (new_width, overlay_height))\n",
    "        \n",
    "        # Split the overlay image into color and alpha channels\n",
    "        overlay_color = overlay_img[:, :, :3]\n",
    "        alpha_channel = overlay_img[:, :, 3] / 255.0\n",
    "        \n",
    "        # Create the output frame with an alpha channel\n",
    "        frame_with_overlay = frame.copy()\n",
    "        overlay_x = (width - overlay_img.shape[1]) // 2  # Center horizontally\n",
    "        overlay_y = height - overlay_img.shape[0]  # Align bottom\n",
    "        \n",
    "        for c in range(3):\n",
    "            frame_with_overlay[overlay_y:overlay_y + overlay_img.shape[0], overlay_x:overlay_x + overlay_img.shape[1], c] = \\\n",
    "                (alpha_channel * overlay_color[:, :, c] + (1 - alpha_channel) * frame_with_overlay[overlay_y:overlay_y + overlay_img.shape[0], overlay_x:overlay_x + overlay_img.shape[1], c])\n",
    "        \n",
    "        output_frames.append(frame_with_overlay)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    # Create video from frames\n",
    "    clip = ImageSequenceClip([cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in output_frames], fps=fps)\n",
    "    \n",
    "    # Add audio to the video\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio_clip = VideoFileClip(audio_path).audio\n",
    "    clip = clip.set_audio(audio_clip)\n",
    "\n",
    "    # Write the final video file\n",
    "    clip.write_videofile(output_video_path, codec='libx264')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is the main loop to get individual overlayed videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  99%|█████████▉| 194/195 [01:48<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide001_sentence001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 200/200 [00:07<00:00, 27.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide001_sentence001.mp4.\n",
      "MoviePy - Writing audio in slide001_sentence001TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide001_sentence001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide001_sentence001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 281/282 [02:35<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide001_sentence002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 286/286 [00:10<00:00, 27.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide001_sentence002.mp4.\n",
      "MoviePy - Writing audio in slide001_sentence002TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide001_sentence002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide001_sentence002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 236/237 [02:08<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide001_sentence003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 242/242 [00:09<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide001_sentence003.mp4.\n",
      "MoviePy - Writing audio in slide001_sentence003TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide001_sentence003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide001_sentence003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 210/211 [01:55<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide002_sentence001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 216/216 [00:07<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide002_sentence001.mp4.\n",
      "MoviePy - Writing audio in slide002_sentence001TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide002_sentence001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide002_sentence001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 248/249 [02:13<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide002_sentence002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 253/253 [00:09<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide002_sentence002.mp4.\n",
      "MoviePy - Writing audio in slide002_sentence002TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide002_sentence002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide002_sentence002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 329/330 [02:59<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide002_sentence003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 335/335 [00:13<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide002_sentence003.mp4.\n",
      "MoviePy - Writing audio in slide002_sentence003TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - Writing video fin_videos\\slide002_sentence003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide002_sentence003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  99%|█████████▉| 183/184 [01:39<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide003_sentence001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 189/189 [00:07<00:00, 26.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide003_sentence001.mp4.\n",
      "MoviePy - Writing audio in slide003_sentence001TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide003_sentence001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide003_sentence001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 247/248 [02:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide003_sentence002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 252/252 [00:10<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide003_sentence002.mp4.\n",
      "MoviePy - Writing audio in slide003_sentence002TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide003_sentence002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide003_sentence002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 365/366 [03:21<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide003_sentence003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 370/370 [00:15<00:00, 24.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide003_sentence003.mp4.\n",
      "MoviePy - Writing audio in slide003_sentence003TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide003_sentence003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide003_sentence003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|█████████▉| 219/220 [02:02<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with storing the frames:  avt_videos\\slide003_sentence004.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 225/225 [00:07<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video fin_videos\\slide003_sentence004.mp4.\n",
      "MoviePy - Writing audio in slide003_sentence004TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video fin_videos\\slide003_sentence004.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready fin_videos\\slide003_sentence004.mp4\n"
     ]
    }
   ],
   "source": [
    "fin_videos = \"fin_videos\"\n",
    "if os.path.exists(fin_videos):\n",
    "    shutil.rmtree(fin_videos)\n",
    "\n",
    "os.mkdir(fin_videos)\n",
    "\n",
    "for avt_file in os.listdir(\"avt_videos\"):\n",
    "    avtr_file = os.path.join(\"avt_videos\",avt_file)\n",
    "    save_frames(input_video_path=avtr_file,threshold=20)\n",
    "    print(\"done with storing the frames: \", avtr_file)\n",
    "    video_path = os.path.join(\"output_videos\",avt_file)\n",
    "    frames_dir = 'frames'\n",
    "    output_video_path = os.path.join(fin_videos,avt_file)\n",
    "    overlay_height = 380\n",
    "    overlay_images_on_video(video_path=video_path, frames_dir = \"frames\", output_video_path=output_video_path, audio_path=avtr_file, overlay_height=overlay_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230.0\n",
      "29.97\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"output_new.mp4\")\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4234.0\n",
      "29.97\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"main.mp4\")\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "def process_and_concat_videos(directory, output_file, seconds_to_remove=0):\n",
    "    # List all video files in the directory\n",
    "    files = sorted([f for f in os.listdir(directory) if f.endswith('.mp4')])\n",
    "    clips = []\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        clip = VideoFileClip(file_path)\n",
    "\n",
    "        # Remove last `seconds_to_remove` seconds if greater than 0\n",
    "        if seconds_to_remove > 0:\n",
    "            duration = clip.duration\n",
    "            if duration > seconds_to_remove:\n",
    "                clip = clip.subclip(0, duration - seconds_to_remove)\n",
    "            else:\n",
    "                print(f\"Video '{file}' is shorter than the specified removal time. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        clips.append(clip)\n",
    "\n",
    "    # Concatenate all video clips\n",
    "    final_clip = concatenate_videoclips(clips, method='compose')\n",
    "\n",
    "    # Write the output file\n",
    "    final_clip.write_videofile(output_file, codec='libx264')\n",
    "\n",
    "# Example usage\n",
    "directory = 'fin_videos'  # Replace with your directory path\n",
    "output_file = 'final_video.mp4'  # Replace with your desired output file name\n",
    "seconds_to_remove = 1  # Number of seconds to remove from the end of each video\n",
    "\n",
    "process_and_concat_videos(directory, output_file, seconds_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
