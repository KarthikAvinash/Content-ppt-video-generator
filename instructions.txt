Meta Llama 3 is the next generation of our state-of-the-art open source large language model. It's available on various platforms including AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake. We're dedicated to developing Llama 3 responsibly, and offering resources to help others do the same. This includes Llama Guard 2, Code Shield, and CyberSec Eval 2 for trust and safety. In the coming months, we expect new capabilities, longer context windows, additional model sizes, and enhanced performance. The Llama 3 research paper will also be released soon.
Introducing Meta Llama 3: The Most Capable Openly Available LLM
Large Language Models
Introducing Meta Llama 3
The Next Generation of Llama
Llama 3 models are here!
This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters, supporting a wide range of use cases. Llama 3 demonstrates state-of-the-art performance on various benchmarks and offers improved reasoning capabilities. We believe these are the best open source models of their class.
[Image of Llama 3 models]
Llama 3 is open source.
We're putting Llama 3 in the hands of the community to kickstart the next wave of AI innovation across the stack—from applications to developer tools and more.
Why Llama 3?
We aimed to build the best open models, matching the quality of proprietary models. We addressed developer feedback to improve Llama 3's helpfulness, prioritizing responsible use and deployment. We're embracing open source ethos by releasing early and often, allowing the community access to models in development.
What's next?
We'll make Llama 3 multilingual and multimodal, enhance its context length, and continue improving overall performance, including reasoning and coding abilities. Our goal is to empower the community with the best possible tools for AI innovation.
Llama 3 Performance
State-of-the-Art Results Across Benchmarks
Llama 3 is a major leap.
Our 8B and 70B parameter Llama 3 models outperform Llama 2 and set a new standard for LLMs at those scales. Improvements in pretraining and post-training have made our pretrained and instruction-fine-tuned models the best at their respective sizes. These improvements have also led to reduced false refusal rates, enhanced alignment, and increased diversity in model responses.
Human evaluations confirm.
We've developed a new high-quality human evaluation set covering 12 key use cases. Our 70B instruction-following model consistently outperforms competing models of similar size in real-world scenarios. The chart below shows aggregated results of human evaluations against Claude Sonnet, Mistral Medium, and GPT-3.5, highlighting Llama 3's strength in real-world tasks.
Focus on real-world use cases.
We evaluated Llama 3 on standard benchmarks and developed a new high-quality human evaluation set, containing 1,800 prompts across 12 key use cases. This set helps assess real-world performance. For improved model safety, even our own modeling teams don't have access to this evaluation set, ensuring that Llama 3 isn't overfitting to it. The chart below shows human evaluation results for Llama 3 against comparable models.
[Chart showing human evaluation results]